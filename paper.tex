%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    INSTITUTE OF PHYSICS PUBLISHING                                   %
%                                                                      %
%   `Preparing an article for publication in an Institute of Physics   %
%    Publishing journal using LaTeX'                                   %
%                                                                      %
%    LaTeX source code `ioplau2e.tex' used to generate `author         %
%    guidelines', the documentation explaining and demonstrating use   %
%    of the Institute of Physics Publishing LaTeX preprint files       %
%    `iopart.cls, iopart12.clo and iopart10.clo'.                      %
%                                                                      %
%    `ioplau2e.tex' itself uses LaTeX with `iopart.cls'                %
%                                                                      %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
% First we have a character check
%
% ! exclamation mark    " double quote  
% # hash                ` opening quote (grave)
% & ampersand           ' closing quote (acute) 
% $ dollar              % percent       
% ( open parenthesis    ) close paren.  
% - hyphen              = equals sign
% | vertical bar        ~ tilde         
% @ at sign             _ underscore
% { open curly brace    } close curly   
% [ open square         ] close square bracket
% + plus sign           ; semi-colon    
% * asterisk            : colon
% < open angle bracket  > close angle   
% , comma               . full stop
% ? question mark       / forward slash 
% \ backslash           ^ circumflex
%
% ABCDEFGHIJKLMNOPQRSTUVWXYZ 
% abcdefghijklmnopqrstuvwxyz 
% 1234567890
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\documentclass[12pt]{iopart}
%\documentclass[12pt]{article}
%\usepackage{amsmath}
%\usepackage{cite}
\usepackage{amssymb,amsfonts}
%\usepackage{algorithmic}
\usepackage{graphicx}
%\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
%\usepackage{graphicx}
%Uncomment next line if AMS fonts required
%\usepackage{iopams}  

\begin{document}
\def\brian[#1]{{\color{red} #1 }}
\def\michael[#1]{{\color{green} #1 }}

\title[Heterogeneous Control of Cell Differentiation Dynamics]{Exploiting Intrinsic Noise for Heterogeneous Cell Control Under Time Delays and Model Uncertainties}
\maketitle

\author{M P May$^1$, B Munsky$^{1,2}$}

\address{$^1$ School of Bioengineering, Colorado State University, Fort Collins, CO, USA}
\address{$^2$ Department of Chemical and Biological Engineering, Colorado State University, Fort Collins, CO, USA}

\begin{abstract}
Previous research has focused to enable robust control performance despite the presence of noise, but understanding how controllers may exploit that noise remains incomplete. 
Motivated by Maxwells-Demon, we previously proposed a cellular control regime in which the exploitation of stochastic noise can break symmetry between and allow for specific control of multiple cells using a single input signal (i.e., single-input-multiple-output or SIMO control).
The current work extends that analysis to include uncertain stochastic systems where system dynamics are are affected by time delays, intrinsic noises, and model uncertainty. 
We find that noise-exploiting controllers can remain highly effective despite coarse approximations to the model's scale or incorrect estimations or extrinsic noise in key model parameters, and these controllers can even retain performance under substantial observer or actuator time delays.  
We also demonstrate how SIMO controllers could drive multi-cell systems to follow different trajectories with different phases and frequencies.
Together, these findings suggest that noise-exploiting control should be possible even in the practical case where models are aways approximate, where parameters are always uncertain, and where observations are corrupted by errors.

\end{abstract}
Keywords: Stochastic Control, Gene Regulation, Optogenetics

%
% Uncomment for keywords
%\vspace{2pc}
%\noindent{\it Keywords}: Stochastic control, optogenetic, synthetic biology
%
% Uncomment for Submitted to journal title message
%\submitto{\JPA}
%
% Uncomment if a separate title page is required
\maketitle
% 
% For two-column output uncomment the next line and choose [10pt] rather than [12pt] in the \documentclass declaration
%\ioptwocol

\section{Introduction}
\begin{figure}
\begin{center}
\includegraphics[width=\columnwidth]{Cartoons.pdf}
\caption{{\bf Single-input-multiple-output (SIMO) control of multiple cells using a single optogenetic input.}
({\bf A}) Schematic of the light-activated genetic system with auto-regulation.
({\bf B}) Diagram of the stochastic SIMO control problem using two optogenetic cells sharing a single input.
({\bf C}) Noise exploiting controllers were optimized to define a fully aware control input (I) and a partially aware control input (III), where the control signal (color scale) depends on the observed expression within the cell or cells (x- and y-axes).  (II and IV) Corresponding steady state marginal distributions for the different cells (red and blue) under these controllers demonstrate a clear a break in symmetry. Dashed lines represent the control target objective. Control performance (RMSE) is show above each distribution.}
\label{cartoons}
\end{center}
\vspace{-0.3in}
\end{figure}

Uncertain fluctuations, or `noise,' is a common theme throughout many fields of engineering, and robust control is a frequent concern when attempting to control any human-made system such as vehicles \cite{Liang2021}, chemical processes \cite{Lucia2013}, or biology \cite{Baetica2020}.
\brian[Please look for some relevant references or a review of control under noise. I highlight several missing refs in red.]
Noise arrises from many sources, occasionally from quantum events and thermally-induced fluctuations, but more commonly from unknown or uncharacterized physical processes, and as such, these fluctuations usually cannot be efficiently reproduced or predicted using purely physical or mechanistic models.
Rather, statistical models are usually employed where key mechanisms or dynamics are subject to stochastic variations or inputs that act as a proxy for random or un-modeled fluctuations elsewhere in the system.
In this context, much effort has been placed on enhancing the robustness of controlled processes, where uncertain or unpredictable variations in internal or external parameters are modeled as intrinsic or extrinsic noise \cite{Bhattacharyya2017}.
For many macroscopic, human-engineered systems, the resulting system dynamics can be modeled effectively simply by combining a deterministic (i.e., noise-free) model with additive noise (usually assumed to be Gaussian under arguments based on the central limit theorem) \cite{Feng2022}, and most current approaches seek to control the system to minimize variations that result from the noisy inputs. 

Analyses of noise and control theory are equally relevant to understand the basic biological processes of gene transcription regulation \cite{Sun2020}or mRNA translation regulation \cite{Cialek2022}or to modify these processes for practical use \cite{Dionisi2022,Baumschlager2017}.
However, at this mesoscopic scale of cellular biology, where transcription factors compete to activate or deactivate individual genes or where mRNA or protein molecules are present at just a few copies (or none at all) per cell, the additive noise model is much less realistic.
In this case, Brownian motion, discrete stochastic gene regulation, and noisy mRNA dynamics collectively generate a fundamentally stochastic environment that cells must effectively manage. 
At this scale, the order or timing of a single reaction event (e.g., the binding of a transcription factor to a promoter) can have dramatic consequences that could last for several cellular generations (e.g., the activation or repression of a gene that promotes unfettered cell growth and differentiation). 
The cell's drive towards homeostasis requires dealing with the inherently chaotic and noisy processes that reside within it, and despite these challenges, cells generally demonstrate strong capability to survive these noisy processes. 
When seeking to understand how such systems evolve or react, the central limit theorem and the Gaussian noise may not apply, and a more detailed statistical analysis of probabilistic behavior is needed uncover hidden properties of cellular control mechanisms.

One emerging field that is particularly dependent on the integration of control and noise is synthetic biology, which aims to develop modular\cite{Ng2019} and orthogonal \cite{Liu2018} components to sense and manipulate \cite{Sheets2020} complex logical systems, enabling them to exhibit a wide range of advanced biological behaviors\cite{Shin2020}. 
Advances in optogenetics have enhanced the ability to reliably actuate embedded systems within cells, offering the potential to exert precise temporal and spatial control on cellular components\cite{Sheets2020,Baumschlager2017,Chen2020,Lillacci2018}.
These developments have facilitated computer-programmable regulation of cellular protein production through external optogenetic inputs and smart microscope techniques\cite{Fox2021,Baumschlager2021,Lugagne2017}. 
These digital-synthetic actuators enable fine-tuned, computer-modulated control of cellular systems, previously unattainable, with faster response times compared to chemical diffusion\cite{Rullan2018, Baumschlager2017} . 
Classical and modern control methods like PID control and model predictive control have been implemented in such systems \cite{Filo2022}to control synthetic systems to different stable points. 

It has recently been shown that new control techniques that leverage the complete probability distribution information of the system could actually harness the noise of single-cell gene regulation to achieve more complicated control objectives. 
For example, inspired by the genetic toggle switch from Kobayashi et al.\cite{Kobayashi2004}, Szymanska et al.~\cite{Szymanska2015} showed that noise could be exploited to achieve independent control of multiple cells using a single input, even despite uncertain parameters or time delays due to maturation of fluorescent proteins or limited observation of the regulatory proteins. 
In May et al.~\cite{May2021}, we identified a simplified stochastic model to reproduce data measured in Baumschlager, et al. \cite{Baumschlager2017} for the expression from a transcription promoter under the optogenetic control of a UV-activated T7 polymerase (see model in Figure \ref{cartoons}A, top promoter).
We then proposed the addition of a positive auto-regulation (Figure \ref{cartoons}A, bottom promoter) to help maintain an elevated expression phenotype in the presence of UV excitation, and we demonstrated how a Single-Input-Multiple-Output (SIMO) multicellular controller could control multiple cells to arbitrary phenotypes using only a single input. 

This paper extends the analysis of the SIMO multicellular control problem by examining the impact of model uncertainties and fluctuating control objectives on the control performance. 
These uncertainties include coarse-grained approximations of the system dynamics, errors or extrinsic variations in the system parameter, and time delays between the observation of the cellular dynamics and actuation of the control process. 
In the following `Methods' we introduce our formulation of the chemical master equation (CME) to analyze the discrete stochastic distribution of cellular responses; we define multiple controllers and demonstrate the computation of their effects on cell dynamics; and we show how the control law can be optimized to improve performance.
In the `Results' section, we explore the how model approximations, parameter inaccuracies, and time delays affect control performance, and we demonstrate a simple scheme for controlling cells to track a dynamically changing reference signal. 
Using discrete stochastic models based on the chemical master equation, we demonstrate that combining biochemical noise, nonlinear auto-regulation, and a single optogenetic feedback could control two genetically identical cells with different initial conditions to follow different desired trajectories at different frequencies and phases. 

\section{Methods}
In May et al. \cite{May2021}, we developed two models for the description of an optogenetically controlled gene expression system.
These first model consisted of six species to describe the light-activated association of two T7 split domains (species 1 and 2) which combine to form an active T7 polymerase (species 3) under optogenetic excitation. 
The active polymerase could then associate with inactive T7 promoters (species 4), resulting in the formation of an active allele (species 5) that could then transcribe and translated to produce the desired protein product (species 6). 
The second, much simpler, single-species model was developed by assuming quasi-steady equilibrium for the first five species. 
Both models were independently parameterized using the same experimental data from Baumschlager et al. \cite{Baumschlager2017}. 
Furthermore, an extension was made to each model to incorporate a secondary self-activated promoter-gene construct, where the expression rate was determined by a Hill function \ref{prodRate}.
Through simulations, we showed that a feedback control law could be designed to force multiple cells to different and individually chosen equilibrium states using a single optogenetic control signal. 
We also showed that when this control law was parametrized using the simple model, it could be used effectively to control the behavior of the more complicated system, thus demonstrating that control performance could remain high despite inaccuracies in the model.
In the current work, we adopt the simpler of the two models and extend our analyses to consider the effects of additional model inaccuracies, including coarse-grained model approximations, parameter errors, and time delays, and we also explore the possibility that a single optogenetic control signal could drive the system to track temporally-changing reference signals.
Section \ref{sec:Model} introduces the model; Section \ref{sec:Stochastic} develops a Master Equation description of the models probabilistic dynamics; Section \ref{sec:Quantification} defines a control objective and optimizes that metric to obtain a baseline control law; and Sections \ref{sec:Scaling} and \ref{sec:Delay} introduce uncertainties into the model related to the granularity of the model approximation or the introduction of time delays, respectively. 

\subsection{Model}\label{sec:Model}
To assess the impact of model approximations on the implementation of noise-enhanced control strategies, we begin with the one-species model proposed by May et al. \cite{May2021} (Figure. \ref{cartoons}A). 
This model comprises two reactions for production and degradation of the key protein. 
The nonlinear, UV-dependent production is defined by the following equation:
\begin{equation}
\nu_1(x,t)= \kappa\frac{x^\eta}{x^\eta+\beta^\eta}+ k_0 + u(UV(t)),
\label{prodRate}
\end{equation}
where $x$ is the instantaneous protein level; $\kappa$ is the maximum strength of the auto-regulation promoter; $\beta$ is the concentration at which auto-regulation promoter reaches its half maximal strength; $\eta$ is the cooperativity in the auto-regulation promoter; $k_0$ is the leakage rate from both promoters; and $u(UV(t))$ is the UV-dependent strength of the T7 promoter. 
Feedback enables the external modulation of the light input using the state of the system, thereby controlling the T7 promoter strength as a function of state rather than time and eliminating the explicit time dependance in $u(UV(t))$.
Protein degradation is assumed to be a first order process with rate $\gamma$:
\begin{equation}
\nu_2(x) = \gamma x.
\end{equation}
All baseline parameters describing the auto-regulation promoter, $\kappa$, $\eta$, $\beta$, and $k_0$, and the degradation rate $\gamma$ are presented in Table 1~\cite{May2021} and are fixed throughout the current study.

\begin{table}[]
\caption{Baseline Model Parameters}
\begin{center}
\begin{tabular}{|c|c|c|l|}
\hline
Parameter & Value   & Units & Meaning                     \\ \hline \hline
$\kappa$         & 0.406   & 1/min & maximal production rate \\ \hline
$\beta$          & 20.0    & molecules  & concentration at half-max activation        \\  \hline
$\eta $           & 8.00    & unitless  & cooperativity factor        \\  \hline
$k_0$           & 0.0001  & 1/min & promote leakage rate \\  \hline
$\gamma$         & 0.0203  & 1/min & degradation/dilution rate         \\  \hline
$u(t)$              & variable & 1/min & applied control signal  \\ \hline
\end{tabular}
\label{table}
\end{center}
\vspace{-0.2in}
\end{table}

\subsection{Stochastic analyses of the model}\label{sec:Stochastic}
To describe the discrete stochastic behavior of the above model for a population of $N_{\rm c}$ cells, we define the current state of the system as the tuple of the non-negative numbers of proteins in each cell: $\mathbf{X}_i = [x_{1},x_{2},\ldots,x_{N_c}]_i\in \mathbb{Z}_{\ge 0}$, where the index $i$ denotes the enumeration of the state within the countably infinite set of all possible states, i.e., $\mathbf{X}_i\in\mathcal{X}=\{\mathbf{X}_1,\mathbf{X}_2,\ldots\}$.
The stoichiometry vector, $\mathbf{s}_{\mu}$, for reaction number $\mu$ is then defined as the change in state following that reaction event (e.g., $\mathbf{X}_i \rightarrow \mathbf{X}_i + \mathbf{s}_\mu$). Specifically, the $2N_c$ possible reactions are defined in pairs corresponding to production ($\mu\in\{1,3,5,\ldots\}$) and degradation ($\mu\in\{2,4,6,\ldots\}$) as:
\begin{eqnarray}
\mathbf{s}_1 &= \mathbf{e}_1,\ \mathbf{s}_2 = -\mathbf{e}_1,\nonumber\\ 
\mathbf{s}_3 &= \mathbf{e}_2,\ \mathbf{s}_4 = -\mathbf{e}_2,\nonumber\\
&\vdots \nonumber\\ 
\mathbf{s}_{\rm 2N_c-1} &= \mathbf{e}_{\rm N_c},\ \mathbf{s}_{\rm 2N_c} = -\mathbf{e}_{\rm N_c}, \label{Stochs}
 \end{eqnarray}
where each $\mathbf{e}_{i}\in\mathbb{Z}^{{\rm N_ c}}$ is is a Euclidean vector (i.e., unity for the $i^{\rm th}$ entry and otherwise zero). The corresponding propensity functions are:
\begin{eqnarray}
w_1(\mathbf{X}) &= u(\mathbf{X},t)  + \kappa \frac{x_1^\eta}{x_1^{\eta}+\beta^{\eta}} + k_0,\ w_2(\mathbf{X}) = \gamma x_1,\nonumber\\
w_3(\mathbf{X}) &= u(\mathbf{X},t)  + \kappa \frac{x_2^\eta}{x_2^{\eta}+\beta^{\eta}} + k_0,\ w_4(\mathbf{X}) = \gamma x_2,\nonumber\\
&\vdots \nonumber\\
w_{\rm 2N_c-1}(\mathbf{X}) &= u(\mathbf{X},t)  + \kappa \frac{x_N^\eta}{x_N^{\eta}+\beta^{\eta}} + k_0,\ w_{\rm 2N_c}(\mathbf{X}) = \gamma x_N.\label{Props}
 \end{eqnarray}

These definitions of the stoichiometry and propensity functions allow us to implement the Gillespie stochastic simulation algorithm (SSA) \cite{Gillespie1992, Gillespie1977} to generate representative trajectories of the stochastic process. At each step, two random numbers are generated to determine the time and the type of the next reaction. Given the current state $\mathbf{X}$, the time until the next reaction is distributed according to an exponential random with rate parameter equal to the inverse of the sum of the propensity functions:
\begin{equation}
Pr(\delta t = \tau) = \sum w_i(\mathbf{X}) \exp\left(- \sum{\mu=1}^{2N_c-1} w_\mu(\mathbf{X})\tau\right),
\end{equation}
and an instance of this random variable, $\delta t$,  can be sampled from this distribution using the expression:
\begin{equation}
\delta t = -\log\left(r_1 \sum_{\mu=1}^{2N_c-1}  w_\mu(\mathbf{X})\right),
\end{equation}
where $r_1$ is a uniform random variable between zero and one.
The probability for the specific individual reaction $R_k$ to fire from all possible reactions is given by:
\begin{equation}
Pr(R_k)=\frac{w_k}{\sum w_{\mu=1}^{2N_c-1} (\mathbf{X})},
\end{equation}
and which specific reaction that fires at time $t+\tau$ is a categorical random variable pulled from $Pr(R_k)$.  The SSA is then simulated by stepping through time one reaction at a time and updating the state of the system by adding the stoichiometry of the reaction to the state of the system.

However, a more direct analysis of the chemical master equation (CME) is necessary to quantify performance and optimize the controller design. The high dimensional CME is a linear ODE that describes the time-dependent changes in probability mass of all possible states. Using the specified reaction propensities and stoichiometries, the CME can be expressed as:
\begin{equation}
\frac{d}{dt}P(\mathbf{X}_i)=\sum_{\mu=1}^{2{\rm N_c}}
\left(
-w_{\mu}(\mathbf{X}_i)P(\mathbf{X}_i)
+w_{\mu}(\mathbf{X}_i-\mathbf{s}_{\mu})P(\mathbf{X}_i-\mathbf{s}_{\mu})
\right).\label{CMEindex}
\end{equation}
For convenience, the CME can also be formulated more compactly in matrix-vector form as:
\begin{equation}
\frac{d}{dt}\mathbf{P}=(\mathbf{A}_0+\textbf{Bu}^{\mathcal{C}})\mathbf{P},\label{CME}
\end{equation}
where $\mathbf{P} = [P(\mathbf{X}_1), P(\mathbf{X}_2), \ldots ]^T$ is the enumerated probability mass vector for all possible states of the system; $\mathbf{A}_0$ is the infinitesimal generator of the stochastic process due to the autoregulation promoter and degradation events;  $\textbf{u}^{\mathcal{C}} =[u^{\mathcal{C}}(\mathbf{X}_1), u^{\mathcal{C}}(\mathbf{X}_2), \ldots ]^T$ is the collection of control inputs associated with each state; and $\textbf{Bu}^{\mathcal{C}}$ is the contribution that these control inputs make to the infinitesimal generator when included into the feedback process. 
 
More specifically, the zero-control infinitesimal generator, $\mathbf{A}_0$, is constructed according to:
\begin{equation}
[\mathbf{A}_0]_{ij} = \left\{
\begin{array}{rl}
-\sum_{\mu =1}^{2N_{\rm c}} w_{\mu}(\mathbf{X}_j), &\textrm{for }i=j,\\
w_{\mu}(\mathbf{X}_j), &\textrm{for }\mathbf{X}_i = \mathbf{X}_j+\mathbf{s}_\mu\\
0, & \textrm{otherwise,}
\end{array}\right. 
\label{InfA}
\end{equation}
and the feedback infinitesimal generator, $\mathbf{Bu}^{\mathcal{C}}$, of the controller is constructed according to
\begin{equation}
[\mathbf{Bu^{\mathcal{C}}]}_{ij} = \left\{
\begin{array}{rl}
- N_{\rm c}{u}^{\mathcal{C}}(\mathbf{X}_j), &\textrm{for }i=j\\
{u}^{\mathcal{C}}(\mathbf{X}_j), &\textrm{for }\begin{array}{ll}\mathbf{X}_i =\mathbf{X}_j + \mathbf{e}_{i_{\rm c}}, \\ \textrm{and } i_{\rm c} = 1,\ldots,N_{\rm c}\end{array},\\
0, & \textrm{otherwise,}
\end{array}\right.
\label{InfB}
\end{equation}
where ${u}^{\mathcal{C}}(\mathbf{X}_j)$ is the specification of the controller, $\mathcal{C}$, in terms of the current instantaneous state, or its partial observations.

For a given controller, the equilibrium distribution of the system ($\mathbf{P}^*$) can be found by solving Eq.\ \ref{CME} and is given by:
\begin{equation}
\mathbf{P}^*={\rm null}(\mathbf{A}+\textbf{Bu}^{\mathcal{C}}).\label{SSDist}
\end{equation}

In principle, the master equation in Eq.\ \ref{CME} could contain an uncountably infinite number of states, and therefore the exact solution as well as the null vector in Eq.\ \ref{SSDist} may not be computable exactly.  To address this issue, we first truncate the system at a finite number for each species and then apply a reflecting boundary condition, resulting in a finite dimensional master equation. To assess the time interval over which this truncation is valid, we also solve the Finite State Projection \cite{Munsky2006} for the same truncation, which allows us to compute an upper bound on the truncation error as a function of time. 

\brian[We should use the FSP to compute the error of the truncation for a couple cases.  We can talk about how to do this when we meet.]
\michael[ok!]

\subsection{Quantification and optimization of control performance}\label{sec:Quantification}
In the SIMO control of stochastic processes, a single input is applied simultaneously to all systems at once, and therefore a control signal that acts beneficially on one cell may destabilize other cells. 
An effective controller must strike a balance among the desired behaviors of all cells in the system. 
To quantify overall performance success, we define the {\em steady state performance error}, $J$, as the expected steady state Euclidean distance of the process from the specified target state, $\mathbf{T}$:
 \begin{equation}
 J = \mathbb{E}\{|\mathbf{X}-\mathbf{T}|_2\}.
 \end{equation}
The squared score is easily calculated by applying a linear operator to the steady state probability distribution $\mathbf{P}^*$ (Eq.\ \ref{SSDist}) as follows:
\begin{eqnarray}
J^2&= \lim_{t\rightarrow \infty}\mathbb{E}\{|\mathbf{X}(t)-\mathbf{T}|_2^2\}, \nonumber \\ 
&=\sum_{i_1,i_2,\ldots} P^*(x_1=i_1,x_2=i_2,\ldots) \left[(i- T_1)^2 + (j- T_2)^2 +\ldots\right],\nonumber  \\
&=\mathbf{C}\mathbf{P}^*,
\label{Euclid} 
\end{eqnarray}
where $\mathbf{C}$ is simply a vector that contains the squared Euclidian distance of each state from the specified target $\mathbf{T}$, i.e., $C_i = |\mathbf{X}_i-\mathbf{T}|_2^2$. 
As a result of this calculation, $J$ is a non-negative scalar that is zero only if $\mathbf{P}^*$ is a delta distribution located exactly at the target vector $\mathbf{T}$.

We consider two controller designs: the fully aware controller ($\mathbf{u}^{FAC}$) that bases its control signal on simultaneous protein count observations from both cells, and the partially aware controller ($\mathbf{u}^{PAC}$) that relies only on observations from a single cell:
\begin{equation}
\mathbf{u}^{FAC}=u(x_1,x_2),
\end{equation}
\begin{equation}
\mathbf{u}^{PAC}=u(x_2),
\end{equation}
where $x_1$ and $x_2$ are discrete integers greater than or equal to zero that represent the instantaneous number of proteins in cell one and cell two. Despite their differences in their observation data, both the FAC and the simpler PAC are optimized to achieve the same goal, namely to drive both cells to their respective set points.

Optimization of $\mathbf{u}^{FAC}$ and $\mathbf{u}^{PAC}$ were performed using a gradient descent method to minimize $J$. Since the square root is a monotonically increasing function, minimizing $J^2$ results in the same control as would minimizing $J$ directly. Therefore, we calculate the negative gradient $-\frac{d(J^2)}{d\mathbf{u}^{(.)}}$ and adjust parameters a small step $d\mathbf{u}^{(.)}$ in that direction. For example, the calculation of the gradient for the FAC controller is given by:
\begin{equation}
\frac{d(J^2)}{d\mathbf{u}^{FAC}}=C \frac{dP}{d\mathbf{u}^{FAC}},
\end{equation}
where $\frac{dP}{d\mathbf{u}^{FAC}}$ can be solved using general minimized residual calculation of
\begin{equation}
A \frac{dP}{d\mathbf{u}^{FAC}}=B \mathbf{u}^{FAC}.
\end{equation}

In \cite{May2021}, controllers were optimized to minimize $J^2$ for a single target state $\mathbf{T} = [10,30]$, but in this work they are extend to different arbitrary target points.


\subsection{Scaling for system granularity}\label{sec:Scaling}


In realistic applications, models are never exact, but are often chosen as simplifications of known processes. 
For example, when analyzing discrete stochastic chemical kinetics, it is common to project the CME onto lower-dimensional spaces using finite state projection \cite{Munsky2006}, time scale separations \cite{Peles2006}, Krylov subspaces \cite{Macnamara2008}, principle orthogonal decompositions \cite{Vo2019}, or other coarse meshes \cite{Munsky2008IEEE,Tapia2012}. 
Similarly, measurements are also always inexact and in many cases may only provide information about relative changes -- for example, although fluorescent proteins usually cannot be counted exactly, one may reasonably assume that a cell's total fluorescence intensity varies linearly with the fluorescent protein concentration. 
To explore how mismatches in the assumed system scale (e.g., arising from model approximations or relative measurements) affect the controllability of the cellular process, we define a granularity parameter ($\alpha=M'/M$) that linearly scales each species' population to increase ($\alpha>1$) or decrease ($\alpha<1$), while maintaining the dynamics and general behavior of the model. 
To apply this granularity parameter, we assume that each propensity function, $w_{\mu}$ (Eqns.\ \ref{Props}) is rescaled to a different level of discreteness by substituting
\begin{equation}\label{eq:granscale}
w'(\mathbf{X})=w(\mathbf{X}/\alpha).
\end{equation}
For example, the production and degradation of protein in cell one would become:
\begin{eqnarray}
w'_1(\mathbf{X}) &= u(\mathbf{X}/\alpha,t)  + \kappa \frac{(x_1/\alpha)^\eta}{(x_1/\alpha)^{\eta}+\beta^{\eta}} + k_0, \nonumber \\
 w'_2(\mathbf{X}) &= \gamma x_1/\alpha. \label{eq:rescaling}
\end{eqnarray}
We note that in order to reuse a control law that has been defined for one level granularity and apply it to a system at another level of granularity, the inputs to the controller must also be scaled by $1/\alpha$ before computing the  control level assigned to the current state, e.g., ${u}^{\mathcal{C}} = {u}^{\mathcal{C}}(\mathbf{X}/\alpha)$. Because identification of the original control formulation, $\mathbf{u}^{FAC}(x_1,x_2)$ and $\mathbf{u}^{PAC}(x_1)$ only considered integer values for $(x_1,x_2)$, control signal values at fractional state values after rescaling $(x_1/\alpha,x_2/\alpha)$ are calculated using 2D cubic interpolation from the control values at the nearest integer state values.
Finally, to provide a consistent metric for relative scoring, the definition of the performance score is also adjusted according to scale magnitudes. For example, in the two cell system the new steady state performance error would become:
\begin{eqnarray}
(J^2)' &= \sum_{i=0}^{M'}  \sum_{j=0}^{M'}P^*(x_1=i,x_2=j) ((i/\alpha - \mathbf T_1)^2 + (j/\alpha -\mathbf T_2)^2),\nonumber \\
& =\mathbf{C}'\mathbf{P}^*.
\label{EuclidV}
\end{eqnarray}
We reiterate that the system parameters and control law were defined and fixed using the the base granularity ($\alpha=1$), and to simulate a practical application where scales may be unknown or variable, these are not recomputed or refit upon changing the system granularity.

\subsection{Observation and actuation time delays}\label{sec:Delay}

Delays are inherent to any realistic control system, and in this case delays would be expected to arise due to the time needed for various biochemical reactions such as the formation of complete polymerases, activation of promoters, transcription and transport of mature mRNA, and the translation and maturation of protein \cite{Cai2007}. 
Additional delays would also arise from data analysis, decision making, and actuation dynamics. 
To investigate the effects of observation or actuation time delays on control performance, we devised a simple time-delay stochastic simulation algorithm. 
This algorithm records the state history after each stochastic reaction, enabling reconstruction of the population history of the species and the time delayed control input propensity.  
Using this information, the time-delayed control signal at time $t$ can be specified as: 

\begin{equation}
u_{\tau}(t)=\left\{
\begin{array}{rl}
      0 ,&\textrm{ for }  t \leq \tau, \\
      {u}^{\mathcal{C}}(x_1(t-\tau), x_2(t-\tau)) , &\textrm{ for }   t > \tau,\\
\end{array}\right. 
\label{timeDelaySSA}
\end{equation}
where $\tau$ is the time delay between observation and actuation, ${u}^{\mathcal{C}}$ is the previously optimized control law (e.g., $\mathbf{u}^{\rm FAC}$ or $\mathbf{u}^{\rm PAC}$), and $x$ gathered from the state history. 
We note that the time delay stochastic process was only simulated using the SSA because to our knowledge an appropriate direct FSP/CME integration procedure has not yet been developed.
\brian[Still missing reference to the extrande method at this website
\url{https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004923}].
\michael[added Voliotis2016 to the section after this comment]
The Extrande method \cite{Voliotis2016} was used to update the control input at an average frequency of 50 updates per minute, far exceeding the dynamics of the system.
\subsection{Tracking Time-Varying Trajectories}\label{sec:Dynamic}
Because optimizing the control law for a static set point as described in Section \ref{sec:Quantification} requires differentiation of the CME (Eq. \ref{CME}) with respect to the control signal at each state, this calculation is approaching the limits of current feasibility.
Extending these calculations to optimize controllers for a dynamically moving set point is much harder and would likely require intractable numerical simulation or development of new mathematical approaches that are beyond the scope of the current study. 
To circumvent this challenge, we instead propose a simple alternative in which the controller sweep though a piecewise constant set of controllers each designed for a specific static target point along the desired trajectory.

Our goal is to control the system to follow a specific target trajectory, $\mathbf{T}(t)$. We choose a discrete set of $K$ target points along this trajectory:
\begin{equation}
\left\{\mathbf{T}_1,\mathbf{T}_2,\ldots,\mathbf{T}_K\right\},
\end{equation}
and for each individual target $\mathbf{T}_i$, we optimize to find a corresponding controller, $\mathbf{u}_i=\mathbf{u}(\mathbf{T}_i)$. Finally, to implement the control at any given time, $t$, we find the index of the nearest precomputed target state:
\begin{equation}
\hat{i}(t) = \textrm{argmin}_i \left| \mathbf{T}(t) -  \mathbf{T}_i\right|_2,
\end{equation}
and assign that controller.
%
%controllers for each target point and cycling through the controllers over time to create the final controller $\mathbf{R}(\mathbf{x},t)$.
%Let $\mathbf{p}(t)$ be a continuous and cyclicly moving point in state space
%\begin{equation}
%\mathbf{p(t)}=[x_1(t),x_2(t)]
%\end{equation}
%where $x_1(t)$, and $x_2(t)$ are both cyclic functions of time $\in \mathbf{R}$.
%Then, $\mathbf{q}$ is a collection of points $q_i$ which evaluate the cyclic function over its natural period (T) in n steps. 
%\begin{equation}
%\mathbf{q}=[q_1,\ldots ,q_n]=[\mathbf{p}(t_1),\ldots,\mathbf{p}(t_n)]
%\end{equation}
%where each $t_i$ comes from linearly spaced points between $0$ and $T$, in $n$ steps 
%\begin{equation}
%t_i = [t_1,\ldots,t_n]=[0,\frac{1}{n} T,\ldots,\frac{n}{n}T]
%\end{equation}
%Then, we optimize a set of controllers which target each $q_i$ in the discretized path.
%\begin{equation}
%\mathbf{U}_i(\mathbf{x})=[\mathbf{u}_1(\mathbf{x}),...,\mathbf{u}_n(\mathbf{x})]
%\end{equation}
%Next, we cycle through each controller at a fixed frequency by using ($\phi(t)$). 
%\begin{equation}
%\phi(t)=\lceil \frac{n}{T} mod(t,T)\rceil = \lceil n \ f \ mod(t,\frac{1}{f})\rceil
%\end{equation}
%where $f$ the cycling frequency given by $1/T$. 
%Then the final control equation is given by 
% \begin{equation}
%\mathbf{R}(\mathbf{x},t)=\bf{U}_{\phi(t)}(x)
%\end{equation}
%Where $\mathbf{R}(\mathbf{x},t)$ returns the control input scalar as a function of state and time. 
Under this piecewise constant controller law, the full time-varying CME becomes
\begin{equation}
\frac{d}{dt}\mathbf{P}=\mathbf{A}+\mathbf{B}\mathbf{u}_{\hat{i}(t)}(\mathbf{x}).
\end{equation}
\section{Results}

%\brian[{\bf We still need this.} Add a paragraph or two to explain the big picture here.  Use figure 1 to remind the reader of (1) the control goals, (2) the control law, (3) the resulting distributions, (4) the objective function and (5) the optimization of the control law to minimize the error.  Refer to the methods for each such calculation.  This section is critical to make sure the reader understands what is coming next.  Then provide an outline of the remaining subsections of the results section so that the reader knows what to expect.]

As developed in the Methods section, we focus on a SIMO controller where {\em all cells receive the same input} at every instant of time. For this, the control signal depends upon the observed state, e.g., $u(t) = \mathbf{u}(\mathbf{x})$ as developed above and illustrated in Fig. \ref{cartoons}(I,III). These controllers have been optimized to break symmetry so that multiple cells can be controlled to different targets as illustrated in Fig. \ref{cartoons}(II,IV), and we quantify performance according to the RMSE error introduced in Eq. \ref{Euclid}. To explore how such controllers may perform in realistic settings where the models are approximate and parameter are unknown or extrinsically variable, we now fix the parameters of those controllers and explore performance robustness to different types of model uncertainties, including parameter errors or variations (Section \ref{sec:ResPars}), incorrect assumptions on system scales (Section \ref{sec:ResGran}) and time delays (Section \ref{sec:ResTime}). Finally, in Section \ref{sec:ResTraj}, we extend the control analysis to consider the performance of the controller for tracking variable reference signals with different frequencies and phases.

 \subsection{Stochastic SIMO Optogenetic Control can Remain Effective Despite Small Parameter Errors or Extrinsic Uncertainties}\label{sec:ResPars}
\begin{figure*}
\begin{center}
\includegraphics[width=1\textwidth]{ParameterPerturbation.pdf}
\caption{Parameter sweeps using the FAC and PAC show a broad range of control performance in cell 1 ({\bf A}),  in cell 2 ({\bf B}), and in both cells ({\bf C}). Columns show each parameter in the model, rows show the cell which has its parameter perturbed. }
\label{Parameter}
\end{center}
\end{figure*}

Real stochastic processes always have unknown or uncertain mechanisms and parameter values. Although model structures and parameter estimates can often be obtained through fitting to training data, these estimates will never be perfect due to unavoidable measurement errors or limited data sets. 
Even with plentiful and precise training data, model and parameter uncertainties are inevitable due to the fact that even genetically identical single-cells exhibit heterogeneity in parameters due to extrinsic variations.
To asses the control performance under such parameter errors, we performed sensitivity analysis on each model parameter for each cell, then on both cells simultaneously. 
For each parameter, $\mathbf{u}^{\rm FAC}$ (solid lines) and $\mathbf{u}^{\rm PAC}$(dashed lines) control performance was examined across a parameter perturbation range from one-tenth to ten-fold of the original values.
Figure \ref{Parameter}A shows the results for these parameter sweeps when a single parameter in Cell 1 is varied (all other parameters are fixed), and Figure.\ \ref{Parameter}B shows the control performance when a single parameter in Cell 2 is varied. 
Finally, \ref{Parameter}C shows the performance change when a single parameter is changed simultaneously in both Cell 1 and Cell 2 at the same time.

Modifications of parameters were found to produce a broad range of effects. 
For example, increasing $\beta$ in Cell 1 quickly worsens performance while increasing $\beta$ in Cell 2 improves performance (compare second column in Figure.\ \ref{Parameter}A and \ref{Parameter}B). 
In some cases, the effects on performance are not monotonic; for example, increasing $\kappa$ in Cell 1 (Figure.\ \ref{Parameter}A, leftmost column) would be highly advantageous up to a limit after which the control performance degrades rapidly. 
In other cases (such as for the promoter leakage rate, $k_0$), the effect of parameter perturbations on performance is insignificant even for relatively large ($s$=10) perturbations, suggesting that this parameter is not important to control performance.
Figure \ref{Parameter}C shows that even when parameters of both Cell 1 and Cell 2 are jointly changed, these changes could also improve or detract from control performance. 
In particular, the analysis shows that control performance could be improved by modifying the system design either to increase the auto-regulation promoter strength ($\kappa$) or its cooperativity ($\eta$) or to decrease the promoter binding constant ($\beta$) or the protein degradation rate ($\gamma$). We note that co-optimization of both the system and the controller law would allow for further improvements to the performance.

\subsection{Controllers trained using one assumed level of granularity can remain effective at other levels of granularity.}\label{sec:ResGran}

We next explored how the granularity of the system would affect the differential control of the multi-cell system. Specifically, we define the level of granularity using the parameter $\alpha$ as described in Section \ref{sec:Scaling} and which affects the propensity functions and scoring according to Eqns.\ \ref{eq:granscale}-\ref{EuclidV}. Effectively, larger $\alpha$ corresponds to systems where larger numbers of individual molecules are needed to achieve the same concentration (e.g., larger volumes), while smaller $\alpha$ corresponds to situations where smaller population sizes can achieve that concentration (e.g., smaller volumes). We previously optimized the controllers $\mathbf{u}^{\rm FAC}$ and $\mathbf{u}^{\rm PAC}$ based on a the default assumption that $\alpha=1$, and we wished to know what would be the consequences if this same controller were to be applied to a system that has a different granularity. 

To explore this tradeoff, control performance scores for the $\mathbf{u}^{\rm FAC}$ and $\mathbf{u}^{\rm PAC}$ controllers were calculated at different levels of granularity ($\alpha$) between 0.2 and 2.0.  Figure \ref{Volume} (A - F) shows the joint probability distributions (left plots) and marginal probability distributions (right plots) of the system at a low granularity ($\alpha=0.2$), the original granularity ($\alpha=1.0$), and at an increased granularity ($\alpha=2.0$). 
%As the system increases, the scale the size of the dynamics of the system becomes stretched over a larger region in state space. 
%This effect can be observed in the smoothness of C with respect to A despite using the same system and the same controller. 
%\brian[the previous two sentences do not make sense to me.]
Figure \ref{Volume}G shows the trend of the performance versus alpha for both the FAC (solid cyan line) and the PAC (solid magenta line) controller.  This improvement in performance appears to approach a small value as the granularity goes to infinity, but since the size of the FSP increases with the square of the system size, systems much larger than $\alpha$=2 (where $\mathbf{A}_0\in \mathbb{R}^{10^4\times10^4}$) become more difficult to calculate using master equation techniques. To bypass this limit in the FSP, sixteen SSA simulations were used to sample the CME of a system with a much larger volume of $\alpha$=100. Each SSA was run for $5\times10^7$ minutes and only the last $4\times10^7$ minutes were sampled to estimate the stationary distribution and calculate the performance score. The performance score estimates of this high granularity SSA using the FAC and PAC were $2.03$ and $5.24$ respectively, which are plotted as dashed lines in Figure.\ \ref{Volume}G. Although it is unclear if further performance improvements could be obtained with further increases to the system scale, for all cases considered so far, we found that both controllers monotonically improved with increased $\alpha$ and that $\mathbf{u}^{\rm FAC}$ always outperforms the $\mathbf{u}^{\rm PAC}$. 

The effect that granularity has on the control performance depends on two competing phenomena: First, because the controller was optimized for one level of granularity ($\alpha = 1$), one might expect that the controller would become worse if the granularity were incorrect. However, at large granularity, we find that the opposite is true - the control performance actually improves when applied to an incorrect model. The reason for this surprising result is that relative amplitude of stochastic fluctuations (i.e., the standard deviation divided by the mean) in a chemical process decreases with the inverse square root of the process scale \cite{VanKampen1992}. In other words, the process becomes more predictable and therefore more controllable. At the extreme as the system size increases, the dynamics converge towards a deterministic process, except for certain exceptional initial conditions lying on manifolds that would separate different steady state behaviors \cite{Strasser2012}. 

However, this improvement in the steady state performance does not come without a cost. Although higher granularity reduces noise and makes it easier to maintain desired states once they have been achieved, noise is necessary to break symmetry between the two cells' dynamics in order to achieve those states in the first place. This tradeoff is illustrated in Fig.\ \ref{Volume}(H), which plots the control performance over time after changing the control goal to exchange the low- and high-target cells with one another.  From the figure, we can see although steady state control performance improves for higher values of $\alpha$, the time taken to reach that steady state performance  increases with $\alpha$, suggesting that larger-volume system may become more susceptible to times delays or less able to track variable reference trajectories.

\begin{figure*}[t!]
\begin{center}
\includegraphics[width=1\textwidth]{Granularity.pdf}
\vspace{-0.1in}
\caption{Systems with increased granularity are less noisy and have better control performance. (A-C) Joint (left) and marginal (right) distributions for the FAC shows increased control performance and tighter distributions as $\alpha$ increases from 0.2 (top row) to 2.0 (bottom row) . (D-F) Joint (left) and marginal (left) distributions using the PAC controller at different levels of granularity.}
\label{Volume}
\end{center}
\vspace{-0.2in}
\end{figure*}


 % XXX - update this to explain what you see in this new figure. 

%These data suggest that increasing granularity increases steady state control performance even if the controller itself depends on noise for its implementation. Also, designing a controller to work for a system of one volume size can result in a controller that works even for another much larger volume.  This has practical significance, because it is relatively easy to search over a control law defined on a small $M\times M$ grid of states (e.g., when $M$=50 as in \cite{May2021}), but to implement the controller for a larger system which may be computationally prohibitive. This result implies that it may be possible to optimize controller using coarse-grained FSP analyses for small granularity problems and adapt these via interpolation for use with larger, more realistic systems, that exceed the computational limit of standard FSP computations.  

\subsection{Heterogeneous control can remain effective despite moderate time delays}\label{sec:ResTime}
\begin{figure*}
\begin{center}
\includegraphics[width=1\textwidth]{TimeDelay.pdf}
\vspace{-0.1in}
\caption{{\bf Effects of time delay on control performance.} (A-C) Joint (left, color scale shown at top) and marginal distributions (right) of the controlled system at different levels of time delay using the FAC controller. The target state, $\mathbf{T}$ is denoted by a small circles on the left panels and dashed lines on the right panels. The steady state RMSE is shown in each case. (D-F) Same as (A-C) but for the PAC controller.  (G) RMSE control performance versus time delay for both FAC (blue) and PAC (green). Letters A-F correspond to panels A-F. Dashed red line corresponds to optimal performance with no feedback (i.e., constant input). Dashed yellow line corresponds to characteristic system time, $\tau_c = 1/\gamma$.}
\label{Time}
\end{center}
\vspace{-0.2in}
\end{figure*}

In general, feedback control can only be effective if one can quickly make measurements, compute adjustments to the control signal, and implement the needed changes within an appropriate amount of time relative to the characteristic timescale of the system. As the time required for any of these steps increases, control performance will be degraded, perhaps even leading to large fluctuations or instability. To explore how time delays affect the noise-enhanced controllers $\mathbf{u}^{\rm FAC}$ and $\mathbf{u}^{\rm PAC}$, we generated large sets of time-delayed stochastic simulations (see Section \ref{sec:Delay}) for different lengths of the time delay. Each SSA was sub-sampled for 1000 times over 10000 minutes of simulation time after a burnin period of 10000 minutes.

Figure \ref{Time} shows the joint distributions (left) and marginal distributions (right) at varying levels of time-delay, with panels A-C showing results for the FAC controller and panels D-F showing results for the PAC controller. Figure \ref{Time}G summarizes these results by plotting the score of both controllers versus the time delay. From the figures, it is clear that performance is rapidly degraded as the delay approaches and then exceeds the characteristic time ($\tau_c = 1 / \gamma = 49$ min) set by the degradation rate of the process (yellow dashed line). At very small time delays ($\tau< 3.4$ min), the FAC outperforms the PAC  but at moderate time delays ($\tau > 3.4$ min) the PAC outperforms the FAC.
%
%These data show that time delays greater than $\tau = 0.1 / \gamma = 4.9$ min begin to degrade FAC control performance and plateau at $\tau_c = 1.0 / \gamma = 49$ min.  \brian[Check these times.] \michael[rewrote the sentence]The choice of best controller depends on the level of time delay in the system.
At high time delays, both controllers lose their asymmetry, resulting in significantly worse performances ($RMSE = 32$ and $23$) and even perform worse than under a simple constant control input without feedback (depicted by a horizontal red line in Fig.\ \ref{Time}G). 
% XXX - This figure needs much larger fonts for all the labels and axes.
% MMM - Did the change
% There are errors in the parameter names.  What is 'alpha'?
% remove the A,B,C,D,E.
% replace I, II, III with A, B, C
% what is parameter $\mu$? Is that an error?  Please use $\mu$ for reaction index as in the methods section.

\begin{figure*}
\begin{center}
\includegraphics[width=1\textwidth]{DelayAndGranularity.pdf}
\vspace{-0.1in}
\caption{{\bf Joint effects of granularity and time delay on control performance.} (A-I) Joint probability distributions at different combinations of $\tau$ (rows) and $\alpha$ (columns). Overall RMSE shown at top. (J) Heat-map of control performance versus $(\tau,\alpha)$. Points A-I correspond to panels A-I. Dashed yellow line shows characteristic time $\tau_c = \alpha/\gamma/$.
\brian[Label 'J' is missing from the figure. Replace XXX below.] \michael[added J. Replace XXX with 4.0]
(K) Controlled system trajectory for $\alpha = 4.0$ and $\tau = $ min (denoted by red star in panel J).}
\label{DG}
\end{center}
\vspace{-0.2in}
\end{figure*}

As discussed in the previous section, increasing the granularity of the system $\alpha$ reduces the randomness to improve the steady state performance but at the cost of slowing down the controller response.  To explore the joint effects of time delays and granularity, Fig,\ \ref{DG}J shows the FAC steady state control performance as a function of the time delay and system granularity $(\tau,\alpha)$ using thirty-two stochastic simulations simulated to steady state at each combination. Control performance errors measured by RMSE improved as $\alpha$ increased when the delays were small ($\tau=1 \ $ min and $\tau=10 \ $ min) but not when $\tau=100 \ $ min. 
%
%\brian[What are the units of tau in Figure5?  This section is confusing in that it is not clear to me how tau and gamma are changing relative to one another when alpha is changed. From Eq 21 it seems that gamma changes with 1/alpha.  If this is correct, does that mean that the critical time changes with $\tau_c = 1/gamma = alpha/gamma ~= 5*alpha$?  If you plot that as a thick dashed dark line in Figure5J,  would it match to the contours? ] \michael[$tc=1/(0.7*alpha)$ matches the counters well]

Recalling that under the system granularity rescaling (Eq.\ \ref{eq:rescaling}), as $\alpha$ changes, the effective degradation rate scales according to $\gamma' = \gamma/\alpha$.  Therefore, the critical limit for the delays should also change with the system scale according to $\tau_c = 1\alpha/\gamma$. Figure\ref{DG}(J) depicts this characteristic line and shows that as the time delay approaches and then exceeds this level, the steady state performance becomes dramatically worse. 

Finally, to understand the model of failure at these longer delays, it is interesting to examine trajectories induced by the controller just below this characteristic delay. For parameter set denoted by the red star in Figure.\ \ref {DG}(J), Figure.\ \ref {DG}(K) shows the controlled response after a long burn in period to achieve steady state. In this case, the application of feedback after a delay leads to a strong oscillatory behavior and worse performance than that achieved without any control at all. This observation further stresses the importance of considering time delays when designing such controllers.

\begin{figure*}
\begin{center}
\includegraphics[width=1\textwidth]{StaticControl.pdf}
\vspace{-0.1in}
\caption{{\bf FAC control laws and performance for different target points.} 
(A) Optimized control input for target $\mathbf{T} = [20,25]$, with target point denoted by star. 
(B) Corresponding steady state response distribution. Marginal distributions for $x_1$ and $x_2$  on right and below.
(C,D) Same as (A,B) but for $\mathbf{T} = [10,25]$.
 (E) FAC control performance (RMSE) versus targets $\mathbf{T} = [T_1,T_2]$.  Stars correspond to target points in panels A-D. Dashed diagonal shows line of symmetry.} 
\label{CtrlP}
\end{center}
\vspace{-0.2in}
\end{figure*}

\subsection{With noise-induced control, a single input can drive multiple cells to follow different temporal trajectories.}\label{sec:ResTraj}

%We next analyzed the control performance over a domain of target points to find if some states are easier to achieve than others. For a two-cell system, stability analysis of the auto-regulated system using a moderate light input can create up to four stable points and one unstable point in the state space.  These points give rise to basins which may attract probability more easily than other points.
We next optimized the FAC controller and calculated its performance for the two-cell system over a two dimensional domain of discrete target points $\mathbf{T}_i = [T_{i1},T_{i2}]$ between between five and forty-five. For example, Fig.\ \ref{CtrlP} (A) shows the optimized FAC control input of the system when the target is $\mathbf{T}=[20,25]$, and Fig.\ \ref{CtrlP} (B) shows the corresponding steady state probability distribution of the controlled system. Figure \ref{CtrlP} (C and D) show the the same thing but for a different target state of $\mathbf{T}=[10, 25]$.
%\brian[Let's remove the PAC from this section.  Show only the FAC and and replace the Figure6C/D with the FAC at a different target point. Put a new star of a different color to match to the new 6C/D.  Remove Figure6F.  Make sure to change all the figure labels and titles and captions.] \michael[ok, I also moved the settpoint score heatmap to the right]
Figure \ref{CtrlP}(E) shows the overall steady state FAC control performance over the entire domain of static set points, and illustrates that some regions are easier to attain than others. 
%For example, in the space contained in the dashed-green box which also hold the four stable points of the system, the control performance is quite strong, whereas near the unstable point (dashed-blue box), the FAC results in relatively worse control. 
%For example, the top left and bottom right edge regions were the most difficult to control, since the control input does not push probability in these directions. \brian[I don't understand this statement.] \michael[editied it better]

\begin{figure*}
\begin{center}
\includegraphics[width=1\textwidth]{DynamicControl.pdf}
\vspace{-0.1in}
\caption{{\bf Tracking time-varying reference signal.}
(A) Schematic of SIMO control to drive two cells to follow different trajectories.
(B1) Reference signal for $x_1$ (red) and $x_2$ (blue).
(B2) Controlled response. Distributions shown in shading. Median shown in lines. Three periods are shown after decay of transient dynamics.
(B3) RMSE performance over time.
(B4) Phase space of reference signal.
(B5) Time-averaged distribution of tracking error.
(C1-C5) Same as (B1-B5) but for phase-lagged reference signal.
(D1-D5) Same as (B1-B5) but for reference signal with two different frequencies and phases.}
\label{CR}
\end{center}
\vspace{-0.2in}
\end{figure*}

We developed a method (Section \ref{sec:Dynamic}) to control the system dynamics to follow a predefined path $\mathbf{T}(t)$ by alternating between 32 different pre-computed controllers along each path. We considered three representative pathways, including an in-sync reference point (Fig.\ \ref{CR}B1) where 
$$
T^{\rm B}_1(t) =20 \sin(2 \pi f t)+10,\textrm{ and } T^{\rm B}_2(t) = 20 \sin(2 \pi f t)+10;
$$
a phase lagged reference point (Fig.\ \ref{CR}C1) where
$$
T^{\rm C}_1(t)=20 \sin(2 \pi f t)+10,\textrm{ and } T^{\rm C}_2(t) =20 \cos(2 \pi f t)+10;
$$
and a frequency separated reference point (Fig.\ \ref{CR}D1) where
$$
T^{\rm D}_1(t)=20 \sin(2 \pi f t)+10,\textrm{ and } T^{\rm D}_2(t) =20 \cos(2 \pi f t)+10.
$$
For each reference signal, the driving frequency was defined as $f=10^{-4}$ cycles per minute.

All FSP simulations were calculated under the time varying control law, and Figs.\ \ref{CR}(B2, C2, and D2) show the corresponding response distributions (shading) and median responses (lines) for two cells $x_1$ and $x_2$ in red and blue, respectively. 
Regions with purple shading depict the exact overlap of red and blue, when both cells have the same distribution of response. 
From Figs.\ \ref{CR}(B2, C2, and D2), we observe that the SIMO control system can effectively drive the system to follow all three input trajectories, including when the two reference trajectories have different phases and frequencies (panel D2). 
To quantify the overall performance, Figs.\ \ref{CR}(B3, C3, and D3) show the root mean squared error as a function of time, and Figs.\ \ref{CR}(B4, C4, and D4) show the time-averaged error distribution for the system response relative to the time varying target. 
From these figures, we observe that all trajectories result in short RMSE spikes during short transient periods when the controller passes through the regions of poor control (i.e., through regions found in Fig.\ \ref{CtrlP}E to have high RMSE errors). Overall, the simulations show that the synchronous control performed best with an average RSME of $6.9$. When the system is driven with a phase-lag, the average RSME of the score increases to $8.2$, and when driven at a different frequency, the average RMSE goes up to $8.6$.  

\begin{figure*}
\begin{center}
\includegraphics[width=1\textwidth]{Frequency.pdf}
\vspace{-0.1in}
\caption{Control performance analyzed over a domain of $f,\alpha$ pairs show worst control performance at moderate frequencies near $1e-2$ due to phase lag. Tracking reference signals when $\alpha=5$ span a range between 50 and 150 species (A). Stochastic simulations driven using a phased lagged controller at $\alpha = 5$ and low frequency show tighter control compared to $\alpha =1$ (B). Systems driven at moderate frequency show worse control performance than high frequency or low frequency (C). Control performance only of phase-lagged system only improves with increasing $\alpha$ and low $f$.}
\label{CRG}
\end{center}
\vspace{-0.2in}
\end{figure*}

In general, a given control system cannot effectively track a reference signal that changes faster that the system's natural time scale. Since it was shown that increasing granularity ($\alpha$) improves steady state control performance (Fig.\ \ref{Time}G) but also lengthens this time scale (Fig.\ \ref{Time}H), one should expect that these competing effects of granularity would also affect the types and speeds of signals to which the system can respond.
To examine how driving frequency and system scale affects control performance, 64 SSA trajectories of the phase-separated dynamic controller were simulated over a two dimensional domain of points $(\alpha,f)$ for 32 cycles after reaching steady state. 
Figure \ref{CRG}(A) shows the desired reference signal for the system, and Fig.\ \ref{CRG}(B) shows the mean of the system response when the frequency is $f=10^{-4}$ cycles/min and the granularity is $\alpha = 4.0$.  
Figure \ref{CRG}(C) shows the corresponding control performance over normalized periodic time as $\alpha$ is held at $4.0$ and $f$ is increased from $f=10^{-4}$ cycles/min to $f=10^{-2}$ cycles/min and $f=10^{-0}$ cycles/min leading to average control performances of 6.2, 16.1, and 15.0 respectively. 
Figure \ref{CRG}D,E extends this analysis to plot the average control performance over different combinations of $\alpha$ and $f$. 
The characteristic frequency (given by the predicted $\tau_c^{-1}$ over a range of $\alpha$) is plotted as the yellow dashed line.
%RMSE control performance of 128 simulations were simulated over a range of frequency between $10^-3$ and $10^-1$ Figure\ref{CRG}(D) for 32 cycles to obtain a better measurement of the true RMSE Figure\ref{CRG}(D) at $\alpha=2.0$, $\alpha=4.0$, and $\alpha=8.0$. 
In particular, the heat map of control performance over $(f,\alpha)$ shows that the worst performance occurs at moderate $f$ near $f=10^{-2}$ cycles/min and that control performance is best when granularity is high and frequency is low \ref{CRG}(E).  More specifically, we find that strong performance was attainable only if the driving frequency was kept lower than a characteristic frequency $f_c \equiv 1 / \tau_c = \gamma/\alpha$, which is denoted by the dashed line in Fig. \ref{CRG}E.


\section{Conclusion}

Noise, whether it arises from inherently stochastic processes or from unknown or unmodeled interactions, can play a critical role in the performance of feedback control. For the field of synthetic biology, this noise has typically been avoided and genetic systems have primarily been engineered to be as robust as possible to these uncertain fluctuations. In contrast, many natural cellular processes exist and thrive in settings where single-molecule events such as gene activation leads to large relative fluctuations, and where response heterogeneity is unavoidable. Key results from \cite{May2021} showed that certain controllers could exploit this noise to achieve objectives that would not be possible in a deterministic setting. With such controllers, a single regulatory signal, such as an optogenetic input, could drive two or more two genetically identical cells to different, arbiltrarilly chosen fates using just a single input signal and irrespective of the cells' initial conditions. 

The effectiveness of any model-based controller depends upon the accuracy of the model with which that controller has been optimized, and as the real system deviates from its idealized model, the control performance will naturally be affected. In this work, we explore the effects of several such deviations, including uncertainties or errors in parameters, mismatches in assumptions for systems scale, and times delays. 

Regarding parameter values, our perturbation analyses (Fig.\ \ref{Parameter}) showed that control performance is strongly affected by the system parameters. 
We found that whether parameters were incorrect in all cells (e.g., due to systematic errors in the model) or for just one cell at a time (e.g., due to extrinsic noise in the cells themselves) affected the control performance in different way. 
In most cases, small changes could be tolerated, whereas large changes, especially to certain key parameters, could be catastrophic. 
Moreover, we found that there can be room to improve control performance by adjusting system parameters, suggesting that joint optimization of the controller with the system itself could lead to even stronger performance.
Armed with such insight into which parameters are the most sensitive and which can safely be ignored, one could in principle focus measurement efforts to more precisely quantify the critical parameters and focus design efforts to reduce variability in key aspects of modular parts.

The size of the system also plays an important role in its control performance. 
For a fixed concentration, as the volume of a chemical reaction system increases, it becomes less noisy, and its dynamics approach that of deterministic process. 
At this limit, symmetry can no longer be broken, and feedback control cannot independently drive different cells to different fates.  
On the other hand, deleterious fluctuations also become smaller, so larger systems can more easily maintained maintain their desired phenotypes.
Overall, we have shown (Fig.\ \ref{Volume}A-G) that the removal of noise through system granularity led to better steady state control performance, but  such systems were found to take a much longer time to achieve steady state (Fig.\ \ref{Volume}H). 
Interestingly, this result could have implications on the malleability of cells at different stages of their growth cycle, where differentiation of smaller cells (e.g., those immediately after division) may be more susceptible to control signals, while larger cells (e.g., mature cells that have already established their phenotypes) may be relatively impervious to external signals.

Although our objective was to determine how well control performance would be maintained under different system sizes, we were surprised to find that controllers designed at one level of granularity (e.g., $\alpha$ =1) worked surprisingly well to control systems at much larger granularities. 
From a practical perspective, the ability to analyze a model at one system scale and then effectively apply it to another could be highly beneficial. 
Since the computation time of the FSP solution to the CME grows with the square of the number of states, this can cause an explosion in computational requirements for large systems. 
Our results suggest a promising alternative in which one could learn or optimize a controller using FSP analyses for a computationally feasible number of states and later apply them to larger systems that cannot be solved using current techniques.

Time delay analysis (Fig. \ref{Time}) showed that increasing time delay decreased control performance. However, we also found that not every controller was equally affected by time delays. In particular, we found that at intermediate and larger time delays, a partially aware controller that has  less information can outperform a fully aware controller (Fig. \ref{Time}G). We believe this is happening because a controller with more information can afford to be more aggressive to implement its control, and time delay can cause this aggression to backfire.

The control of cells to two slowly changing dynamic reference signals using a single global input by the use of a noise-exploiting controller showed good control performance for a variety of signals. These analysis could be extended to include faster frequency by numerical calculation or by alternative error-probability adjustments.



\section*{References}
\bibliographystyle{unsrt}
\bibliography{refs}
\end{document}

